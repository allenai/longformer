{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aad_1s7ybD5o"
   },
   "source": [
    "# `RoBERTa` --> `Longformer`: build a \"long\" version of pretrained models\n",
    "\n",
    "This notebook replicates the procedure descriped in the [Longformer paper](https://arxiv.org/abs/2004.05150) to train a Longformer model starting from the RoBERTa checkpoint. The same procedure can be applied to build the \"long\" version of other pretrained models as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BieXv0YUd7NF"
   },
   "source": [
    "### Data, libraries, and imports\n",
    "Our procedure requires a corpus for pretraining. For demonstration, we will use Wikitext103; a corpus of 100M tokens from wikipedia articles. Depending on your application, consider using a different corpus that is a better match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AZZ4VmKSwvH"
   },
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "!unzip wikitext-103-raw-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3yjIYKXw3rL"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0NnMMl6wy7Q"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast, RobertaModel, TextDataset, DataCollatorForLanguageModeling, Trainer\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from transformers import LongformerSelfAttention\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgoNVJYUbD59"
   },
   "source": [
    "### RobertaLong\n",
    "\n",
    "`RobertaLongForMaskedLM` represents the \"long\" version of the `RoBERTa` model. It replaces `BertSelfAttention` with `RobertaLongSelfAttention`, which is a thin wrapper around `LongformerSelfAttention`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9EBISkRxPjO"
   },
   "outputs": [],
   "source": [
    "class RobertaLongSelfAttention(LongformerSelfAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :class:`LongformerSelfAttention` expects `len(hidden_states)` to be multiple of `attention_window`. Padding to\n",
    "        `attention_window` happens in :meth:`LongformerModel.forward` to avoid redoing the padding on each layer.\n",
    "\n",
    "        Modified LongformerSelfAttention layer to work with RoBERTa models. They have different set of arguments\n",
    "        compared to original Longformer attention: RoBERTa is passing some additional arguments and arguments required\n",
    "        by LongformerSelfAttention are missing so we have to add it manually.\n",
    "\n",
    "        The `attention_mask` is changed in :meth:`LongformerModel.forward` from 0, 1, 2 to:\n",
    "\n",
    "            * -10000: no attention\n",
    "            * 0: local attention\n",
    "            * +10000: global attention\n",
    "        \"\"\"\n",
    "        hidden_states = hidden_states.transpose(0, 1)\n",
    "\n",
    "        # project hidden states\n",
    "        query_vectors = self.query(hidden_states)\n",
    "        key_vectors = self.key(hidden_states)\n",
    "        value_vectors = self.value(hidden_states)\n",
    "\n",
    "        ## Lines below has been changed\n",
    "\n",
    "        attention_mask = attention_mask.squeeze(dim=2).squeeze(dim=1)\n",
    "\n",
    "        # is index masked or global attention\n",
    "        is_index_masked = attention_mask < 0\n",
    "        is_index_global_attn = attention_mask > 0\n",
    "        is_global_attn = any(is_index_global_attn.flatten())\n",
    "\n",
    "        ## End of modification\n",
    "\n",
    "        seq_len, batch_size, embed_dim = hidden_states.size()\n",
    "        assert (\n",
    "            embed_dim == self.embed_dim\n",
    "        ), f\"hidden_states should have embed_dim = {self.embed_dim}, but has {embed_dim}\"\n",
    "\n",
    "        # normalize query\n",
    "        query_vectors /= math.sqrt(self.head_dim)\n",
    "\n",
    "        query_vectors = query_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        key_vectors = key_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        attn_scores = self._sliding_chunks_query_key_matmul(\n",
    "            query_vectors, key_vectors, self.one_sided_attn_window_size\n",
    "        )\n",
    "\n",
    "        # values to pad for attention probs\n",
    "        \n",
    "        ## Lines below has been changed\n",
    "        \n",
    "        remove_from_windowed_attention_mask = (attention_mask != 0).unsqueeze(dim=-1).unsqueeze(dim=-1)\n",
    "\n",
    "        ## End of modification\n",
    "\n",
    "        # cast to fp32/fp16 then replace 1's with -inf\n",
    "        float_mask = remove_from_windowed_attention_mask.type_as(query_vectors).masked_fill(\n",
    "            remove_from_windowed_attention_mask, -10000.0\n",
    "        )\n",
    "        # diagonal mask with zeros everywhere and -inf inplace of padding\n",
    "        diagonal_mask = self._sliding_chunks_query_key_matmul(\n",
    "            float_mask.new_ones(size=float_mask.size()), float_mask, self.one_sided_attn_window_size\n",
    "        )\n",
    "\n",
    "        # pad local attention probs\n",
    "        attn_scores += diagonal_mask\n",
    "\n",
    "        assert list(attn_scores.size()) == [\n",
    "            batch_size,\n",
    "            seq_len,\n",
    "            self.num_heads,\n",
    "            self.one_sided_attn_window_size * 2 + 1,\n",
    "        ], f\"local_attn_probs should be of size ({batch_size}, {seq_len}, {self.num_heads}, {self.one_sided_attn_window_size * 2 + 1}), but is of size {attn_scores.size()}\"\n",
    "\n",
    "        # compute local attention probs from global attention keys and contact over window dim\n",
    "        if is_global_attn:\n",
    "            # compute global attn indices required through out forward fn\n",
    "            (\n",
    "                max_num_global_attn_indices,\n",
    "                is_index_global_attn_nonzero,\n",
    "                is_local_index_global_attn_nonzero,\n",
    "                is_local_index_no_global_attn_nonzero,\n",
    "            ) = self._get_global_attn_indices(is_index_global_attn)\n",
    "            # calculate global attn probs from global key\n",
    "\n",
    "            global_key_attn_scores = self._concat_with_global_key_attn_probs(\n",
    "                query_vectors=query_vectors,\n",
    "                key_vectors=key_vectors,\n",
    "                max_num_global_attn_indices=max_num_global_attn_indices,\n",
    "                is_index_global_attn_nonzero=is_index_global_attn_nonzero,\n",
    "                is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero,\n",
    "                is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero,\n",
    "            )\n",
    "            # concat to local_attn_probs\n",
    "            # (batch_size, seq_len, num_heads, extra attention count + 2*window+1)\n",
    "            attn_scores = torch.cat((global_key_attn_scores, attn_scores), dim=-1)\n",
    "\n",
    "            # free memory\n",
    "            del global_key_attn_scores\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1, dtype=torch.float32)  # use fp32 for numerical stability\n",
    "\n",
    "        # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n",
    "        attn_probs = torch.masked_fill(attn_probs, is_index_masked[:, :, None, None], 0.0)\n",
    "        attn_probs = attn_probs.type_as(attn_scores)\n",
    "\n",
    "        # free memory\n",
    "        del attn_scores\n",
    "\n",
    "        # apply dropout\n",
    "        attn_probs = F.dropout(attn_probs, p=self.dropout, training=self.training)\n",
    "\n",
    "        value_vectors = value_vectors.view(seq_len, batch_size, self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        # compute local attention output with global attention value and add\n",
    "        if is_global_attn:\n",
    "            # compute sum of global and local attn\n",
    "            attn_output = self._compute_attn_output_with_global_indices(\n",
    "                value_vectors=value_vectors,\n",
    "                attn_probs=attn_probs,\n",
    "                max_num_global_attn_indices=max_num_global_attn_indices,\n",
    "                is_index_global_attn_nonzero=is_index_global_attn_nonzero,\n",
    "                is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero,\n",
    "            )\n",
    "        else:\n",
    "            # compute local attn only\n",
    "            attn_output = self._sliding_chunks_matmul_attn_probs_value(\n",
    "                attn_probs, value_vectors, self.one_sided_attn_window_size\n",
    "            )\n",
    "\n",
    "        assert attn_output.size() == (batch_size, seq_len, self.num_heads, self.head_dim), \"Unexpected size\"\n",
    "        attn_output = attn_output.transpose(0, 1).reshape(seq_len, batch_size, embed_dim).contiguous()\n",
    "\n",
    "        # compute value for global attention and overwrite to attention output\n",
    "        # TODO: remove the redundant computation\n",
    "        if is_global_attn:\n",
    "            global_attn_output, global_attn_probs = self._compute_global_attn_output_from_hidden(\n",
    "                hidden_states=hidden_states,\n",
    "                max_num_global_attn_indices=max_num_global_attn_indices,\n",
    "                is_local_index_global_attn_nonzero=is_local_index_global_attn_nonzero,\n",
    "                is_index_global_attn_nonzero=is_index_global_attn_nonzero,\n",
    "                is_local_index_no_global_attn_nonzero=is_local_index_no_global_attn_nonzero,\n",
    "                is_index_masked=is_index_masked,\n",
    "            )\n",
    "\n",
    "            # get only non zero global attn output\n",
    "            nonzero_global_attn_output = global_attn_output[\n",
    "                is_local_index_global_attn_nonzero[0], :, is_local_index_global_attn_nonzero[1]\n",
    "            ]\n",
    "\n",
    "            # overwrite values with global attention\n",
    "            attn_output[is_index_global_attn_nonzero[::-1]] = nonzero_global_attn_output.view(\n",
    "                len(is_local_index_global_attn_nonzero[0]), -1\n",
    "            )\n",
    "            # The attention weights for tokens with global attention are\n",
    "            # just filler values, they were never used to compute the output.\n",
    "            # Fill with 0 now, the correct values are in 'global_attn_probs'.\n",
    "            attn_probs[is_index_global_attn_nonzero] = 0\n",
    "\n",
    "        outputs = (attn_output.transpose(0, 1),)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (attn_probs,)\n",
    "\n",
    "        return outputs + (global_attn_probs,) if (is_global_attn and output_attentions) else outputs\n",
    "\n",
    "\n",
    "class RobertaLongForMaskedLM(RobertaForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.roberta.encoder.layer = nn.ModuleList([RobertaLongSelfAttention(config, layer_id=index) for index in range(config.num_hidden_layers)])\n",
    "\n",
    "class RobertaLongModel(RobertaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder.layer = nn.ModuleList([RobertaLongSelfAttention(config, layer_id=index) for index in range(config.num_hidden_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LRZa5s1bD6E"
   },
   "source": [
    "Starting from the `roberta-base` checkpoint, the following function converts it into an instance of `RobertaLong`. It makes the following changes:\n",
    "\n",
    "- extend the position embeddings from `512` positions to `max_pos`. In Longformer, we set `max_pos=4096`\n",
    "\n",
    "- initialize the additional position embeddings by copying the embeddings of the first `512` positions. This initialization is crucial for the model performance (check table 6 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) for performance without this initialization)\n",
    "\n",
    "- replaces `modeling_bert.BertSelfAttention` objects with `modeling_longformer.LongformerSelfAttention` with a attention window size `attention_window`\n",
    "\n",
    "The output of this function works for long documents even without pretraining. Check tables 6 and 11 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) to get a sense of the expected performance of this model before pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-m4A_ttixPuf"
   },
   "outputs": [],
   "source": [
    "def create_long_model(\n",
    "    initialization_model,\n",
    "    initialization_tokenizer,\n",
    "    save_model_to,\n",
    "    attention_window,\n",
    "    max_pos\n",
    "):\n",
    "    model = RobertaForMaskedLM.from_pretrained(initialization_model)\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(initialization_tokenizer, model_max_length=max_pos)\n",
    "    config = model.config\n",
    "\n",
    "    # extend position embeddings\n",
    "    tokenizer.model_max_length = max_pos\n",
    "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
    "    current_max_pos, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n",
    "    max_pos += 2  # NOTE: RoBERTa has positions 0,1 reserved, so embedding size is max position + 2\n",
    "    config.max_position_embeddings = max_pos\n",
    "    assert max_pos > current_max_pos\n",
    "\n",
    "    # allocate a larger position embedding matrix\n",
    "    new_pos_embed = model.roberta.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
    "\n",
    "    # copy position embeddings over and over to initialize the new position embeddings\n",
    "    k = 2\n",
    "    step = current_max_pos - 2\n",
    "    while k < max_pos - 1:\n",
    "        new_pos_embed[k:(k + step)] = model.roberta.embeddings.position_embeddings.weight[2:]\n",
    "        k += step\n",
    "    model.roberta.embeddings.position_embeddings.weight.data = new_pos_embed\n",
    "    model.roberta.embeddings.position_ids.data = torch.tensor([i for i in range(max_pos)]).reshape(1, max_pos) # v4.0.0+\n",
    "\n",
    "    # replace the `modeling_bert.BertSelfAttention` object with `RobertaLongSelfAttention`\n",
    "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        longformer_self_attn = RobertaLongSelfAttention(config, layer_id=i)\n",
    "        longformer_self_attn.query = copy.deepcopy(layer.attention.self.query)\n",
    "        longformer_self_attn.key = copy.deepcopy(layer.attention.self.key)\n",
    "        longformer_self_attn.value = copy.deepcopy(layer.attention.self.value)\n",
    "\n",
    "        longformer_self_attn.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "        longformer_self_attn.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "        longformer_self_attn.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "\n",
    "        layer.attention.self = longformer_self_attn\n",
    "\n",
    "    logger.info(f'saving model to {save_model_to}')\n",
    "    model.save_pretrained(save_model_to)\n",
    "    tokenizer.save_pretrained(save_model_to)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkbQvhOMbD6L"
   },
   "source": [
    "Pretraining on Masked Language Modeling (MLM) doesn't update the global projection layers. After pretraining, the following function copies `query`, `key`, `value` to their global counterpart projection matrices.\n",
    "For more explanation on \"local\" vs. \"global\" attention, please refer to the documentation [here](https://huggingface.co/transformers/model_doc/longformer.html#longformer-self-attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CO3MoEgCxP9W"
   },
   "outputs": [],
   "source": [
    "def copy_proj_layers(model):\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        layer.query_global = copy.deepcopy(layer.query)\n",
    "        layer.key_global = copy.deepcopy(layer.key)\n",
    "        layer.value_global = copy.deepcopy(layer.value)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KjI9f8BbD6S"
   },
   "source": [
    "### Pretrain and Evaluate on masked language modeling (MLM)\n",
    "\n",
    "The following function pretrains and evaluates a model on MLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wC2sddeLyVhA"
   },
   "outputs": [],
   "source": [
    "def pretrain_and_evaluate(args, model, tokenizer, eval_only, model_path, max_size):\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                              file_path=args.val_datapath,\n",
    "                              block_size=max_size)\n",
    "    if eval_only:\n",
    "        train_dataset = val_dataset\n",
    "    else:\n",
    "        logger.info(f'Loading and tokenizing training data is usually slow: {args.train_datapath}')\n",
    "        train_dataset = TextDataset(tokenizer=tokenizer,\n",
    "                                    file_path=args.train_datapath,\n",
    "                                    block_size=max_size,)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=True,\n",
    "        mlm_probability=0.15,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    \n",
    "    eval_loss = trainer.evaluate()\n",
    "    eval_loss = eval_loss['eval_loss']\n",
    "    logger.info(f'Initial eval bpc: {eval_loss/math.log(2)}')\n",
    "    \n",
    "    if not eval_only:\n",
    "        trainer.train(model_path=model_path)\n",
    "        trainer.save_model()\n",
    "\n",
    "        eval_loss = trainer.evaluate()\n",
    "        eval_loss = eval_loss['eval_loss']\n",
    "        logger.info(f'Eval bpc after pretraining: {eval_loss/math.log(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqY_Hg5HbD6a"
   },
   "source": [
    "**Training hyperparameters**\n",
    "\n",
    "- Following RoBERTa pretraining setting, we set number of tokens per batch to be `2^18` tokens. Changing this number might require changes in the lr, lr-scheudler, #steps and #warmup steps. Therefor, it is a good idea to keep this number constant.\n",
    "\n",
    "- Note that: `#tokens/batch = batch_size x #gpus x gradient_accumulation x seqlen`\n",
    "   \n",
    "- In [the paper](https://arxiv.org/pdf/2004.05150.pdf), we train for 65k steps, but 3k is probably enough (check table 6)\n",
    "\n",
    "- **Important note**: The lr-scheduler in [the paper](https://arxiv.org/pdf/2004.05150.pdf) is polynomial_decay with power 3 over 65k steps. To train for 3k steps, use a constant lr-scheduler (after warmup). Both lr-scheduler are not supported in HF trainer, and at least **constant lr-scheduler** will need to be added. \n",
    "\n",
    "- Pretraining will take 2 days on 1 x 32GB GPU with fp32. Consider using fp16 and using more gpus to train faster (if you increase `#gpus`, reduce `gradient_accumulation` to maintain `#tokens/batch` as mentioned earlier).\n",
    "\n",
    "- As a demonstration, this notebook is training on wikitext103 but wikitext103 is rather small that it takes 7 epochs to train for 3k steps Consider doing a single epoch on a larger dataset (800M tokens) instead.\n",
    "\n",
    "- Set #gpus using `CUDA_VISIBLE_DEVICES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zl_hDDlryVo2"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=4096, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "parser = HfArgumentParser((TrainingArguments, ModelArgs,))\n",
    "\n",
    "\n",
    "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
    "    '--output_dir', 'tmp',\n",
    "    '--warmup_steps', '500',\n",
    "    '--learning_rate', '0.00003',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--adam_epsilon', '1e-6',\n",
    "    '--max_steps', '3000',\n",
    "    '--logging_steps', '500',\n",
    "    '--save_steps', '500',\n",
    "    '--max_grad_norm', '5.0',\n",
    "    '--per_device_eval_batch_size', '2',\n",
    "    '--per_device_train_batch_size', '2',  # 32GB gpu with fp32\n",
    "    '--gradient_accumulation_steps', '2',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "])\n",
    "training_args.val_datapath = 'wikitext-103-raw/wiki.valid.raw'\n",
    "training_args.train_datapath = 'wikitext-103-raw/wiki.train.raw'\n",
    "\n",
    "# Choose GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi5ZIKcsbD6e"
   },
   "source": [
    "### Put it all together\n",
    "\n",
    "1) Evaluating `roberta-base` on MLM to establish a baseline. Validation `bpc` = `2.536` which is higher than the `bpc` values in table 6 [here](https://arxiv.org/pdf/2004.05150.pdf) because wikitext103 is harder than our pretraining corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b59172f252764a3cae42171d649fa160"
     ]
    },
    "colab_type": "code",
    "id": "JiKj7D1c1ovy",
    "outputId": "dd430ba8-c69b-4110-e384-550a7b4875e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Evaluating roberta-base (seqlen: 512) for refernece ...\n",
      "INFO:filelock:Lock 140107232885488 acquired on wikitext-103-raw/cached_lm_RobertaTokenizerFast_510_wiki.valid.raw.lock\n",
      "INFO:filelock:Lock 140107232885488 released on wikitext-103-raw/cached_lm_RobertaTokenizerFast_510_wiki.valid.raw.lock\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [245/245 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initial eval bpc: 2.5170049634562996\n"
     ]
    }
   ],
   "source": [
    "model_name = 'roberta-base'\n",
    "roberta_base = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "roberta_base_tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "logger.info('Evaluating roberta-base (seqlen: 512) for refernece ...')\n",
    "\n",
    "pretrain_and_evaluate(\n",
    "    training_args,\n",
    "    roberta_base,\n",
    "    roberta_base_tokenizer,\n",
    "    eval_only=True,\n",
    "    model_path=None,\n",
    "    max_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBXU3r69bD6l"
   },
   "source": [
    "2) As descriped in `create_long_model`, convert a `roberta-base` model into `roberta-base-4096` which is an instance of `RobertaLong`, then save it to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7tcsSfZ1-b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converting roberta-base into roberta-base-4096\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:saving model to roberta-base-4096\n"
     ]
    }
   ],
   "source": [
    "model_path = f'roberta-base-{model_args.max_pos}'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "logger.info(f'Converting roberta-base into roberta-base-{model_args.max_pos}')\n",
    "model, tokenizer = create_long_model(\n",
    "    save_model_to=model_path,\n",
    "    initialization_model=model_name,\n",
    "    initialization_tokenizer=model_name,\n",
    "    attention_window=model_args.attention_window,\n",
    "    max_pos=model_args.max_pos,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JiftMH3-zPUS"
   },
   "source": [
    "3) Load `roberta-base-4096` from the disk. This model works for long sequences even without pretraining. If you don't want to pretrain, you can stop here and start finetuning your `roberta-base-4096` on downstream tasks ðŸŽ‰ðŸŽ‰ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8vNeYdrzMd2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from roberta-base-4096\n",
      "Some weights of the model checkpoint at roberta-base-4096 were not used when initializing RobertaLongForMaskedLM: ['roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query_global.weight', 'roberta.encoder.layer.0.attention.self.query_global.bias', 'roberta.encoder.layer.0.attention.self.key_global.weight', 'roberta.encoder.layer.0.attention.self.key_global.bias', 'roberta.encoder.layer.0.attention.self.value_global.weight', 'roberta.encoder.layer.0.attention.self.value_global.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query_global.weight', 'roberta.encoder.layer.1.attention.self.query_global.bias', 'roberta.encoder.layer.1.attention.self.key_global.weight', 'roberta.encoder.layer.1.attention.self.key_global.bias', 'roberta.encoder.layer.1.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.value_global.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.query_global.weight', 'roberta.encoder.layer.2.attention.self.query_global.bias', 'roberta.encoder.layer.2.attention.self.key_global.weight', 'roberta.encoder.layer.2.attention.self.key_global.bias', 'roberta.encoder.layer.2.attention.self.value_global.weight', 'roberta.encoder.layer.2.attention.self.value_global.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query_global.weight', 'roberta.encoder.layer.3.attention.self.query_global.bias', 'roberta.encoder.layer.3.attention.self.key_global.weight', 'roberta.encoder.layer.3.attention.self.key_global.bias', 'roberta.encoder.layer.3.attention.self.value_global.weight', 'roberta.encoder.layer.3.attention.self.value_global.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query_global.weight', 'roberta.encoder.layer.4.attention.self.query_global.bias', 'roberta.encoder.layer.4.attention.self.key_global.weight', 'roberta.encoder.layer.4.attention.self.key_global.bias', 'roberta.encoder.layer.4.attention.self.value_global.weight', 'roberta.encoder.layer.4.attention.self.value_global.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.query_global.bias', 'roberta.encoder.layer.5.attention.self.key_global.weight', 'roberta.encoder.layer.5.attention.self.key_global.bias', 'roberta.encoder.layer.5.attention.self.value_global.weight', 'roberta.encoder.layer.5.attention.self.value_global.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query_global.weight', 'roberta.encoder.layer.6.attention.self.query_global.bias', 'roberta.encoder.layer.6.attention.self.key_global.weight', 'roberta.encoder.layer.6.attention.self.key_global.bias', 'roberta.encoder.layer.6.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.value_global.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query_global.weight', 'roberta.encoder.layer.7.attention.self.query_global.bias', 'roberta.encoder.layer.7.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.key_global.bias', 'roberta.encoder.layer.7.attention.self.value_global.weight', 'roberta.encoder.layer.7.attention.self.value_global.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.query_global.weight', 'roberta.encoder.layer.8.attention.self.query_global.bias', 'roberta.encoder.layer.8.attention.self.key_global.weight', 'roberta.encoder.layer.8.attention.self.key_global.bias', 'roberta.encoder.layer.8.attention.self.value_global.weight', 'roberta.encoder.layer.8.attention.self.value_global.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.query_global.weight', 'roberta.encoder.layer.9.attention.self.query_global.bias', 'roberta.encoder.layer.9.attention.self.key_global.weight', 'roberta.encoder.layer.9.attention.self.key_global.bias', 'roberta.encoder.layer.9.attention.self.value_global.weight', 'roberta.encoder.layer.9.attention.self.value_global.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.query_global.weight', 'roberta.encoder.layer.10.attention.self.query_global.bias', 'roberta.encoder.layer.10.attention.self.key_global.weight', 'roberta.encoder.layer.10.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.value_global.weight', 'roberta.encoder.layer.10.attention.self.value_global.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query_global.weight', 'roberta.encoder.layer.11.attention.self.query_global.bias', 'roberta.encoder.layer.11.attention.self.key_global.weight', 'roberta.encoder.layer.11.attention.self.key_global.bias', 'roberta.encoder.layer.11.attention.self.value_global.weight', 'roberta.encoder.layer.11.attention.self.value_global.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing RobertaLongForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaLongForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaLongForMaskedLM were not initialized from the model checkpoint at roberta-base-4096 and are newly initialized: ['roberta.encoder.layer.0.query.weight', 'roberta.encoder.layer.0.query.bias', 'roberta.encoder.layer.0.key.weight', 'roberta.encoder.layer.0.key.bias', 'roberta.encoder.layer.0.value.weight', 'roberta.encoder.layer.0.value.bias', 'roberta.encoder.layer.0.query_global.weight', 'roberta.encoder.layer.0.query_global.bias', 'roberta.encoder.layer.0.key_global.weight', 'roberta.encoder.layer.0.key_global.bias', 'roberta.encoder.layer.0.value_global.weight', 'roberta.encoder.layer.0.value_global.bias', 'roberta.encoder.layer.1.query.weight', 'roberta.encoder.layer.1.query.bias', 'roberta.encoder.layer.1.key.weight', 'roberta.encoder.layer.1.key.bias', 'roberta.encoder.layer.1.value.weight', 'roberta.encoder.layer.1.value.bias', 'roberta.encoder.layer.1.query_global.weight', 'roberta.encoder.layer.1.query_global.bias', 'roberta.encoder.layer.1.key_global.weight', 'roberta.encoder.layer.1.key_global.bias', 'roberta.encoder.layer.1.value_global.weight', 'roberta.encoder.layer.1.value_global.bias', 'roberta.encoder.layer.2.query.weight', 'roberta.encoder.layer.2.query.bias', 'roberta.encoder.layer.2.key.weight', 'roberta.encoder.layer.2.key.bias', 'roberta.encoder.layer.2.value.weight', 'roberta.encoder.layer.2.value.bias', 'roberta.encoder.layer.2.query_global.weight', 'roberta.encoder.layer.2.query_global.bias', 'roberta.encoder.layer.2.key_global.weight', 'roberta.encoder.layer.2.key_global.bias', 'roberta.encoder.layer.2.value_global.weight', 'roberta.encoder.layer.2.value_global.bias', 'roberta.encoder.layer.3.query.weight', 'roberta.encoder.layer.3.query.bias', 'roberta.encoder.layer.3.key.weight', 'roberta.encoder.layer.3.key.bias', 'roberta.encoder.layer.3.value.weight', 'roberta.encoder.layer.3.value.bias', 'roberta.encoder.layer.3.query_global.weight', 'roberta.encoder.layer.3.query_global.bias', 'roberta.encoder.layer.3.key_global.weight', 'roberta.encoder.layer.3.key_global.bias', 'roberta.encoder.layer.3.value_global.weight', 'roberta.encoder.layer.3.value_global.bias', 'roberta.encoder.layer.4.query.weight', 'roberta.encoder.layer.4.query.bias', 'roberta.encoder.layer.4.key.weight', 'roberta.encoder.layer.4.key.bias', 'roberta.encoder.layer.4.value.weight', 'roberta.encoder.layer.4.value.bias', 'roberta.encoder.layer.4.query_global.weight', 'roberta.encoder.layer.4.query_global.bias', 'roberta.encoder.layer.4.key_global.weight', 'roberta.encoder.layer.4.key_global.bias', 'roberta.encoder.layer.4.value_global.weight', 'roberta.encoder.layer.4.value_global.bias', 'roberta.encoder.layer.5.query.weight', 'roberta.encoder.layer.5.query.bias', 'roberta.encoder.layer.5.key.weight', 'roberta.encoder.layer.5.key.bias', 'roberta.encoder.layer.5.value.weight', 'roberta.encoder.layer.5.value.bias', 'roberta.encoder.layer.5.query_global.weight', 'roberta.encoder.layer.5.query_global.bias', 'roberta.encoder.layer.5.key_global.weight', 'roberta.encoder.layer.5.key_global.bias', 'roberta.encoder.layer.5.value_global.weight', 'roberta.encoder.layer.5.value_global.bias', 'roberta.encoder.layer.6.query.weight', 'roberta.encoder.layer.6.query.bias', 'roberta.encoder.layer.6.key.weight', 'roberta.encoder.layer.6.key.bias', 'roberta.encoder.layer.6.value.weight', 'roberta.encoder.layer.6.value.bias', 'roberta.encoder.layer.6.query_global.weight', 'roberta.encoder.layer.6.query_global.bias', 'roberta.encoder.layer.6.key_global.weight', 'roberta.encoder.layer.6.key_global.bias', 'roberta.encoder.layer.6.value_global.weight', 'roberta.encoder.layer.6.value_global.bias', 'roberta.encoder.layer.7.query.weight', 'roberta.encoder.layer.7.query.bias', 'roberta.encoder.layer.7.key.weight', 'roberta.encoder.layer.7.key.bias', 'roberta.encoder.layer.7.value.weight', 'roberta.encoder.layer.7.value.bias', 'roberta.encoder.layer.7.query_global.weight', 'roberta.encoder.layer.7.query_global.bias', 'roberta.encoder.layer.7.key_global.weight', 'roberta.encoder.layer.7.key_global.bias', 'roberta.encoder.layer.7.value_global.weight', 'roberta.encoder.layer.7.value_global.bias', 'roberta.encoder.layer.8.query.weight', 'roberta.encoder.layer.8.query.bias', 'roberta.encoder.layer.8.key.weight', 'roberta.encoder.layer.8.key.bias', 'roberta.encoder.layer.8.value.weight', 'roberta.encoder.layer.8.value.bias', 'roberta.encoder.layer.8.query_global.weight', 'roberta.encoder.layer.8.query_global.bias', 'roberta.encoder.layer.8.key_global.weight', 'roberta.encoder.layer.8.key_global.bias', 'roberta.encoder.layer.8.value_global.weight', 'roberta.encoder.layer.8.value_global.bias', 'roberta.encoder.layer.9.query.weight', 'roberta.encoder.layer.9.query.bias', 'roberta.encoder.layer.9.key.weight', 'roberta.encoder.layer.9.key.bias', 'roberta.encoder.layer.9.value.weight', 'roberta.encoder.layer.9.value.bias', 'roberta.encoder.layer.9.query_global.weight', 'roberta.encoder.layer.9.query_global.bias', 'roberta.encoder.layer.9.key_global.weight', 'roberta.encoder.layer.9.key_global.bias', 'roberta.encoder.layer.9.value_global.weight', 'roberta.encoder.layer.9.value_global.bias', 'roberta.encoder.layer.10.query.weight', 'roberta.encoder.layer.10.query.bias', 'roberta.encoder.layer.10.key.weight', 'roberta.encoder.layer.10.key.bias', 'roberta.encoder.layer.10.value.weight', 'roberta.encoder.layer.10.value.bias', 'roberta.encoder.layer.10.query_global.weight', 'roberta.encoder.layer.10.query_global.bias', 'roberta.encoder.layer.10.key_global.weight', 'roberta.encoder.layer.10.key_global.bias', 'roberta.encoder.layer.10.value_global.weight', 'roberta.encoder.layer.10.value_global.bias', 'roberta.encoder.layer.11.query.weight', 'roberta.encoder.layer.11.query.bias', 'roberta.encoder.layer.11.key.weight', 'roberta.encoder.layer.11.key.bias', 'roberta.encoder.layer.11.value.weight', 'roberta.encoder.layer.11.value.bias', 'roberta.encoder.layer.11.query_global.weight', 'roberta.encoder.layer.11.query_global.bias', 'roberta.encoder.layer.11.key_global.weight', 'roberta.encoder.layer.11.key_global.bias', 'roberta.encoder.layer.11.value_global.weight', 'roberta.encoder.layer.11.value_global.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path}')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
    "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kS0Np2F4bD6p"
   },
   "source": [
    "4) Pretrain `roberta-base-4096` for `3k` steps, each steps has `2^18` tokens. Notes: \n",
    "\n",
    "- The `training_args.max_steps = 3 ` is just for the demo. **Remove this line for the actual training**\n",
    "\n",
    "- Training for `3k` steps will take 2 days on a single 32GB gpu with `fp32`. Consider using `fp16` and more gpus to train faster. \n",
    "\n",
    "- Tokenizing the training data the first time is going to take 5-10 minutes.\n",
    "\n",
    "- MLM validation `bpc` **before** pretraining: **2.652**, a bit worse than the **2.536** of `roberta-base`. As discussed in [the paper](https://arxiv.org/pdf/2004.05150.pdf) this is expected because the model didn't learn yet to work with the sliding window attention. \n",
    "\n",
    "- MLM validation `bpc` after pretraining for a few number of steps: **2.628**. It is quickly getting better. By 3k steps, it should be better than the **2.536** of `roberta-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f2d298034ac44de397e262fd9902f4cf",
      "dbff0a67283b41c7acc81c9bc7b3c3d2",
      "b8cfa9900fe745f58430dfbd1e86f25e",
      "21cf3ef186aa4895aa9741376834a84e"
     ]
    },
    "colab_type": "code",
    "id": "SHD7QMUWbD6q",
    "outputId": "684d79ed-2237-4c7d-f0b9-dd6ba6b45d00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pretraining roberta-base-4096 ... \n",
      "INFO:filelock:Lock 140107416535104 acquired on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.valid.raw.lock\n",
      "INFO:filelock:Lock 140107416535104 released on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.valid.raw.lock\n",
      "INFO:__main__:Loading and tokenizing training data is usually slow: wikitext-103-raw/wiki.train.raw\n",
      "INFO:filelock:Lock 140107416536592 acquired on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.train.raw.lock\n",
      "INFO:filelock:Lock 140107416536592 released on wikitext-103-raw/cached_lm_RobertaTokenizerFast_4094_wiki.train.raw.lock\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='62' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 02:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initial eval bpc: 17.58360666208166\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:16, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Eval bpc after pretraining: 17.489237717761426\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Pretraining roberta-base-{model_args.max_pos} ... ')\n",
    "\n",
    "training_args.max_steps = 3   ## <<<<<<<<<<<<<<<<<<<<<<<< REMOVE THIS <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "pretrain_and_evaluate(\n",
    "    training_args,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_only=False,\n",
    "    model_path=training_args.output_dir,\n",
    "    max_size=model_args.max_pos,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GOc0_6SbD6u"
   },
   "source": [
    "5) Copy global projection layers. MLM pretraining doesn't train global projections, so we need to call `copy_proj_layers` to copy the local projection layers to the global ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obupoA0FbD6v",
    "outputId": "059fa657-a3f4-401d-8230-a9997ffaed67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Copying local projection layers into global projection layers ... \n",
      "INFO:__main__:Saving model to roberta-base-4096\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Copying local projection layers into global projection layers ... ')\n",
    "model = copy_proj_layers(model)\n",
    "logger.info(f'Saving model to {model_path}')\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtZ2sfWkbD60"
   },
   "source": [
    "ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰ **DONE**. ðŸŽ‰ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "`model` can now be used for finetuning on downstream tasks after loading it from the disk. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOBmcHTj3NPz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from roberta-base-4096\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path}')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path)\n",
    "model = RobertaLongForMaskedLM.from_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "convert_model_to_long.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
